{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Q-learning\n",
        "\n",
        "### Model (Q-Learning):\n",
        "The reinforcement learning model used is Q-learning, a classic algorithm for learning optimal action-selection policies in Markov decision processes. The Q-table stores Q-values for state-action pairs, and the agent updates these values based on observed rewards and transitions. During training, the agent balances exploration and exploitation to discover optimal policies.\n",
        "\n",
        "### Environment (Dataset):\n",
        "The code uses the OpenAI Gym toolkit, and specifically, the Taxi-v3 environment. This environment represents a simplified taxi problem where an agent must pick up and drop off passengers while navigating a grid. The environment provides discrete states, actions, and rewards, making it suitable for reinforcement learning.\n",
        "\n",
        "### Code Overview:\n",
        "The provided code consists of two main parts: training the Q-learning agent and evaluating its performance.\n",
        "\n",
        "#### Training the Agent:\n",
        "1. **Initialization:** The Q-table is initialized with zeros. This table will store Q-values for state-action pairs, representing the expected cumulative rewards.\n",
        "\n",
        "2. **Training Loop:**\n",
        "   - The agent explores the environment by taking actions and updating Q-values based on the Q-learning formula.\n",
        "   - Hyperparameters like learning rate (`alpha`), discount factor (`gamma`), and exploration-exploitation trade-off (`epsilon`) are defined.\n",
        "   - The training loop runs for a specified number of episodes, during which the agent interacts with the environment, learns from experiences, and updates its Q-table.\n",
        "\n",
        "#### Evaluating the Agent:\n",
        "1. The trained agent is evaluated over a specified number of episodes to assess its performance.\n",
        "2. The agent selects actions based on learned Q-values (exploitation).\n",
        "3. Metrics such as average timesteps per episode and average penalties per episode are calculated and printed.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-KlEbe5-LqFT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jgm-RZurDpk4",
        "outputId": "809a8594-b5a9-4d7e-f62a-0c2eca704e78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (3.27.9)\n",
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.10/dist-packages (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym[atari]) (0.0.8)\n",
            "Collecting ale-py~=0.7.5 (from gym[atari])\n",
            "  Downloading ale_py-0.7.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.10/dist-packages (from ale-py~=0.7.5->gym[atari]) (6.1.1)\n",
            "Installing collected packages: ale-py\n",
            "Successfully installed ale-py-0.7.5\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "# Use pip to install the 'cmake' package, which is a cross-platform build tool.\n",
        "!pip install cmake\n",
        "\n",
        "# Install the 'gym' package with the 'atari' extra dependencies.\n",
        "# 'gym' is a toolkit for developing and comparing reinforcement learning algorithms.\n",
        "# The '[atari]' indicates that additional dependencies for Atari environments will be installed.\n",
        "!pip install 'gym[atari]'\n",
        "\n",
        "# Install the 'scipy' package, which is a scientific library for mathematics, science, and engineering.\n",
        "!pip install scipy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import necessary libraries\n",
        "import gym\n",
        "import random\n",
        "import numpy as np\n",
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "OHlPGAbuMQDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Taxi environment and extract the inner environment.\n",
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "# Reset the environment to a new, random state.\n",
        "env.reset()\n",
        "\n",
        "# Print the current state of the environment in ANSI mode.\n",
        "print(env.render(mode=\"ansi\"))\n",
        "\n",
        "# Print the action space of the environment.\n",
        "print(\"Action Space {}\".format(env.action_space))\n",
        "\n",
        "# Print the state space of the environment.\n",
        "print(\"State Space {}\".format(env.observation_space))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-I5FpyiEEDS",
        "outputId": "fe03475c-f651-46a1-8a22-a581dccb9cce"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "\n",
            "\n",
            "Action Space Discrete(6)\n",
            "State Space Discrete(500)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:43: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n",
            "See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The parameters represent (taxi row, taxi column, passenger index, destination index).\n",
        "state = env.encode(3, 1, 2, 0)\n",
        "print(\"State:\", state)\n",
        "\n",
        "# Set the environment's current state to the encoded state.\n",
        "env.s = state\n",
        "\n",
        "# Print the rendered state of the environment in ANSI mode.\n",
        "env.render(mode=\"ansi\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "MybCp8ueEXJC",
        "outputId": "b69e1a66-b31a-4b7b-8b08-49c976b16573"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "State: 328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'+---------+\\n|\\x1b[35mR\\x1b[0m: | : :\\x1b[34;1mG\\x1b[0m|\\n| : | : :\\x1b[43m \\x1b[0m|\\n| : : : : |\\n| | : | : |\\n|Y| : |B: |\\n+---------+\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env.P[328]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_j_JW7WAEdf7",
        "outputId": "ed4c7674-aca5-4468-a802-d4c91c656967"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the environment to the specified state for illustration.\n",
        "env.s = 328\n",
        "\n",
        "# Initialize variables for tracking the number of epochs, penalties, and total rewards.\n",
        "epochs = 0\n",
        "penalties, reward = 0, 0\n",
        "\n",
        "# Create a list to store frames for animation.\n",
        "frames = []\n",
        "\n",
        "# Flag indicating whether the episode is done.\n",
        "done = False\n",
        "\n",
        "# Run the episode until it is done.\n",
        "while not done:\n",
        "    # Choose a random action from the action space.\n",
        "    action = env.action_space.sample()\n",
        "\n",
        "    # Take a step in the environment based on the chosen action.\n",
        "    state, reward, done, info = env.step(action)\n",
        "\n",
        "    # If the agent receives a penalty, increment the penalties counter.\n",
        "    if reward == -10:\n",
        "        penalties += 1\n",
        "\n",
        "    # Put each rendered frame into a dictionary for animation.\n",
        "    frames.append({\n",
        "        'frame': env.render(mode='ansi'),\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "    })\n",
        "\n",
        "    # Increment the epoch counter.\n",
        "    epochs += 1\n",
        "\n",
        "# Print the results of the episode.\n",
        "print(\"Timesteps taken: {}\".format(epochs))\n",
        "print(\"Penalties incurred: {}\".format(penalties))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IbhOrzpwEqMI",
        "outputId": "d304967f-df5e-4e1e-9cbe-2c0654a26762"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Timesteps taken: 1756\n",
            "Penalties incurred: 575\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "from io import StringIO\n",
        "\n",
        "# Function to print frames for animation\n",
        "def print_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        # Clear the output for a dynamic display.\n",
        "        clear_output(wait=True)\n",
        "\n",
        "        # Use StringIO to read the frame and print it.\n",
        "        file = StringIO(frame['frame'])\n",
        "        print(file.getvalue())\n",
        "\n",
        "        # Print additional information for each frame.\n",
        "        print(f\"Timestep: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "\n",
        "        # Pause for a short duration between frames.\n",
        "        sleep(.1)\n",
        "\n",
        "# Call the function to display the frames.\n",
        "print_frames(frames)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5pTdF50LFaSV",
        "outputId": "5f4bf7df-6ecf-4d81-8bfa-24d662e5d1bb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Timestep: 1756\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize a Q-table with zeros.\n",
        "# The shape of the table is determined by the number of states and actions in the environment.\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
      ],
      "metadata": {
        "id": "9TT7A3gCFotb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "alpha = 0.1  # Learning rate\n",
        "gamma = 0.6  # Discount factor\n",
        "epsilon = 0.1  # Exploration-exploitation trade-off\n",
        "\n",
        "# For plotting metrics\n",
        "all_epochs = []\n",
        "all_penalties = []\n",
        "\n",
        "# Training loop\n",
        "for i in range(1, 100001):\n",
        "    # Reset the environment for a new episode\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Exploration-exploitation trade-off\n",
        "        if random.uniform(0, 1) < epsilon:\n",
        "            action = env.action_space.sample()  # Explore action space\n",
        "        else:\n",
        "            action = np.argmax(q_table[state])  # Exploit learned values\n",
        "\n",
        "        # Take a step in the environment\n",
        "        next_state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Update Q-values using the Q-learning formula\n",
        "        old_value = q_table[state, action]\n",
        "        next_max = np.max(q_table[next_state])\n",
        "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "        q_table[state, action] = new_value\n",
        "\n",
        "        # Update counters\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        state = next_state\n",
        "        epochs += 1\n",
        "\n",
        "    # Logging for every 100 episodes\n",
        "    if i % 100 == 0:\n",
        "        clear_output(wait=True)\n",
        "        print(f\"Episode: {i}\")\n",
        "\n",
        "print(\"Training finished.\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHIYFMDbF9rl",
        "outputId": "66c84865-e4d5-45a1-9252-1eee52cb8c3c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode: 100000\n",
            "Training finished.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize variables to track total epochs and penalties over episodes.\n",
        "total_epochs, total_penalties = 0, 0\n",
        "\n",
        "# Number of episodes for evaluation.\n",
        "episodes = 100\n",
        "\n",
        "# Run the agent for the specified number of episodes.\n",
        "for _ in range(episodes):\n",
        "    # Reset the environment for a new episode.\n",
        "    state = env.reset()\n",
        "    epochs, penalties, reward = 0, 0, 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Choose actions using the learned Q-values (exploitation).\n",
        "        action = np.argmax(q_table[state])\n",
        "        state, reward, done, info = env.step(action)\n",
        "\n",
        "        # Count penalties incurred during the episode.\n",
        "        if reward == -10:\n",
        "            penalties += 1\n",
        "\n",
        "        # Increment the timestep counter.\n",
        "        epochs += 1\n",
        "\n",
        "    # Update total counters for all episodes.\n",
        "    total_penalties += penalties\n",
        "    total_epochs += epochs\n",
        "\n",
        "# Calculate and print average metrics over all episodes.\n",
        "print(f\"Results after {episodes} episodes:\")\n",
        "print(f\"Average timesteps per episode: {total_epochs / episodes}\")\n",
        "print(f\"Average penalties per episode: {total_penalties / episodes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9u2ZOaFGQa_",
        "outputId": "cf42f239-5d14-4af1-cdfc-a6191993fa45"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results after 100 episodes:\n",
            "Average timesteps per episode: 12.53\n",
            "Average penalties per episode: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Q-learning agent was trained over 100,000 episodes on the Taxi-v3 environment. The training process took approximately 1756 timesteps per episode, and the agent incurred 575 penalties. During the final episode of training, the agent took 1756 timesteps, reached state 0, took action 5, and received a reward of 20.\n",
        "\n",
        "Upon evaluation over 100 episodes:\n",
        "- Average timesteps per episode: 12.53\n",
        "- Average penalties per episode: 0.0\n",
        "\n",
        "**Conclusion:**\n",
        "1. **Training Efficiency:** The agent efficiently learned a policy to navigate the Taxi environment, completing episodes in a relatively small number of steps.\n",
        "\n",
        "2. **Penalties:** The agent successfully learned a policy with no penalties during evaluation episodes, indicating effective navigation.\n",
        "\n",
        "3. **Generalization:** The Q-learning model demonstrated good generalization, achieving a low average penalty rate in unseen scenarios. The average timesteps per episode is a positive indicator of the agent's ability to learn an effective policy."
      ],
      "metadata": {
        "id": "0Cb2lAjBNMmP"
      }
    }
  ]
}